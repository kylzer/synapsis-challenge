services:
  chat:
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    command: uvicorn main:app --host 0.0.0.0 --port 3000 --reload
    mem_limit: 1g
    ports:
      - 3001:3000
    networks:
      - ollama-docker
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    restart: always
    mem_limit: 4g
    ports:
      - 11434:11434
    environment:
      - OLLAMA_MODELS=/usr/share/ollama/.ollama/models
    volumes:
      - ollama_data:/usr/share/ollama/.ollama/models
    networks:
      - ollama-docker
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull llama3.2:3b && \
      wait"]

networks:
  ollama-docker:
    external: false

volumes:
  ollama_data:
